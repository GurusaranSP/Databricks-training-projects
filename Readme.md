# ðŸ“˜ Databricks 14 Days AI Challenge

**End-to-End Lakehouse, Analytics & ML Training (Day 0 â€“ Day 14)**

## ðŸ“Œ Overview

This repository/document captures my complete learning journey through the **Databricks 14 Days AI Challenge**, organized by **Indian Data Club** in collaboration with **Codebasics**, and sponsored by **Databricks**.

The objective of this program was to build **strong practical foundations** in:

*   Databricks Lakehouse architecture  
    
*   Apache Spark & Delta Lake  
    
*   Data engineering best practices  
    
*   Governance & performance optimization  
    
*   SQL analytics & dashboards  
    
*   Statistical analysis & machine learning  
    
*   MLflow experiment tracking  
    
*   AI-powered analytics concepts  
    

All work was performed using **Databricks Community Edition**, with clear handling of platform limitations where applicable.

## ðŸ§± Dataset Used

*   **E-commerce Behavior Dataset (2019 â€“ Oct & Nov)  
    **
*   Source: Kaggle  
    
*   Scale: ~13M+ events  
    
*   Event types: view, cart, purchase, remove\_from\_cart  
    

This dataset was incrementally processed using **Bronze â†’ Silver â†’ Gold** layers.

## ðŸ“‚ Project Structure (Day-wise)

Each day corresponds to a dedicated notebook/script focusing on a specific concept.

## ðŸŸ¦ Day 0 â€“ Environment Setup & Data Loading

**Objective:** Prepare Databricks workspace and ingest raw data.

**Key Activities:**

*   Created Databricks Community Edition account  
    
*   Configured Kaggle API credentials  
    
*   Created schemas and volumes  
    
*   Downloaded and extracted raw CSV data  
    
*   Loaded Oct & Nov 2019 data into Spark DataFrames  
    

**Outcome:  
**A reproducible, production-style ingestion setup.

## ðŸŸ¦ Day 1 â€“ PySpark Basics

**Objective:** Understand Spark DataFrames and basic operations.

**Key Concepts:**

*   Spark vs Pandas  
    
*   DataFrame creation  
    
*   Schema inspection  
    
*   Filtering and simple transformations  
    

**Outcome:  
**Comfort with basic PySpark syntax and execution model.

## ðŸŸ¦ Day 2 â€“ Apache Spark Fundamentals

**Objective:** Learn Sparkâ€™s core abstractions and transformations.

**Key Concepts:**

*   Spark architecture (driver, executors, DAG)  
    
*   Lazy evaluation  
    
*   DataFrames vs RDDs  
    
*   SQL temporary views  
    

**Outcome:  
**Ability to perform real analytical queries using Spark SQL and DataFrames.

## ðŸŸ¦ Day 3 â€“ PySpark Transformations Deep Dive

**Objective:** Perform complex data transformations.

**Key Concepts:**

*   Joins (inner, left, right, outer)  
    
*   Window functions (ranking, cumulative counts)  
    
*   Aggregations and pivots  
    
*   Feature bucketing and derived columns  
    

**Outcome:  
**Hands-on experience with advanced Spark transformations used in real pipelines.

## ðŸŸ¦ Day 4 â€“ Delta Lake Basics

**Objective:** Introduce Delta Lake and ACID reliability.

**Key Concepts:**

*   Delta vs Parquet  
    
*   ACID transactions  
    
*   Schema enforcement  
    
*   Managed tables  
    

**Outcome:  
**Reliable, transactional data storage using Delta Lake.

## ðŸŸ¦ Day 5 â€“ Delta Lake Advanced

**Objective:** Work with advanced Delta features.

**Key Concepts:**

*   Time Travel  
    
*   MERGE (upserts)  
    
*   OPTIMIZE and ZORDER  
    
*   VACUUM for cleanup  
    

**Outcome:  
**Understanding of how Delta Lake supports incremental and performant pipelines.

## ðŸŸ¦ Day 6 â€“ Medallion Architecture

**Objective:** Design a production-grade data pipeline.

**Layers Implemented:**

*   **Bronze:** Raw ingestion with audit metadata  
    
*   **Silver:** Cleaned, deduplicated, validated data  
    
*   **Gold:** Aggregated, business-ready datasets  
    

**Outcome:  
**Clear separation of concerns and scalable pipeline design.

## ðŸŸ¦ Day 7 â€“ Databricks Jobs & Orchestration

**Objective:** Automate pipelines using jobs.

**Key Concepts:**

*   Notebook parameters (dbutils.widgets)  
    
*   Multi-task job orchestration  
    
*   Task dependencies (Bronze â†’ Silver â†’ Gold)  
    
*   Job controller notebook  
    
*   Error handling and job exits  
    

**Outcome:  
**End-to-end automated pipeline execution.

## ðŸŸ¦ Day 8 â€“ Unity Catalog & Governance

**Objective:** Apply data governance concepts.

**Key Concepts:**

*   Catalog â†’ Schema â†’ Table hierarchy  
    
*   Managed vs external tables  
    
*   Access control (GRANT / REVOKE)  
    
*   Controlled views  
    
*   Lineage awareness  
    

**Outcome:  
**Governed, discoverable, and secure data access.

## ðŸŸ¦ Day 9 â€“ SQL Analytics & Dashboards

**Objective:** Perform analytics using SQL.

**Key Concepts:**

*   Analytical SQL queries  
    
*   Revenue analysis  
    
*   Funnels and conversion rates  
    
*   Aggregations for dashboards  
    

**Outcome:  
**Business-focused insights derived directly from Gold tables.

## ðŸŸ¦ Day 10 â€“ Performance Optimization

**Objective:** Improve query performance.

**Key Concepts:**

*   Query execution plans  
    
*   Partitioning strategies  
    
*   OPTIMIZE & ZORDER  
    
*   Benchmarking  
    
*   Caching considerations (CE-aware)  
    

**Outcome:  
**Ability to reason about and improve Spark performance.

## ðŸŸ¦ Day 11 â€“ Statistical Analysis & ML Preparation

**Objective:** Prepare data for machine learning.

**Key Concepts:**

*   Descriptive statistics  
    
*   Hypothesis testing (weekday vs weekend)  
    
*   Correlation analysis  
    
*   Feature engineering  
    
*   Time-based features  
    

**Outcome:  
**ML-ready feature set derived from clean data.

## ðŸŸ¦ Day 12 â€“ MLflow Basics

**Objective:** Track ML experiments.

**Key Concepts:**

*   MLflow runs  
    
*   Parameter & metric logging  
    
*   Model logging  
    
*   Handling missing labels (NaN / NULL)  
    

**Outcome:  
**Reproducible ML experimentation workflow.

## ðŸŸ¦ Day 13 â€“ Model Comparison & Feature Engineering

**Objective:** Compare multiple ML models.

**Key Concepts:**

*   Linear Regression, Decision Trees, Random Forest  
    
*   RÂ² metric comparison  
    
*   Feature importance  
    
*   Spark ML pipelines  
    
*   Handling NULL labels in Spark ML  
    

**Outcome:  
**Model selection based on metrics and data understanding.

## ðŸŸ¦ Day 14 â€“ AI-Powered Analytics (Conceptual + Practical)

**Objective:** Understand AIâ€™s role in modern analytics.

**Key Concepts:**

*   Databricks Genie (NL â†’ SQL concept)  
    
*   Mosaic AI overview  
    
*   AI-assisted analytics  
    
*   Simple classification using Spark ML  
    
*   MLflow logging for AI workflows  
    

**Note:  
**Full Genie and Mosaic AI features require paid Databricks workspaces; Community Edition limitations were handled transparently.

**Outcome:  
**Clear understanding of how GenAI integrates with governed data platforms.

## ðŸš€ What Comes Next: Capstone Project (Codebasics)

With the 14-day foundation complete, the next phase is the **Codebasics Capstone Project**, where:

*   A real-world problem statement will be provided  
    
*   End-to-end data engineering, analytics, and ML will be applied  
    
*   Best practices learned here will be consolidated into a production-style project  
    

This training phase was intentionally focused on **depth, correctness, and fundamentals**, not shortcuts.

## ðŸŽ¯ Key Takeaways

*   Built a complete **Lakehouse pipeline** from raw data to AI insights  
    
*   Understood **why** things break, not just how to fix them  
    
*   Learned to work within **real platform constraints  
    **
*   Developed habits aligned with **industry-grade data engineering  
    **

## ðŸ“Ž Acknowledgements

*   **Databricks** â€“ Platform & learning ecosystem  
    
*   **Indian Data Club** â€“ Community & challenge organization  
    
*   **Codebasics** â€“ Structured learning and capstone phase  
    

ðŸ“Œ _This document represents a complete hands-on learning journey from Day 0 to Day 14 and serves as the foundation for the upcoming capstone project._



